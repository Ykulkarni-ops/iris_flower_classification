{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tliPDFkdVIV1",
        "outputId": "b4b7a23c-00ec-4ac6-a664-64c369035271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq /content/drive/MyDrive/DS_Assignment_internship.zip"
      ],
      "metadata": {
        "id": "kqKRRS7aV9Sl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install striprtf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhci8enbXBu_",
        "outputId": "f4ee748c-b07a-443f-e24f-fd380f3f81c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting striprtf\n",
            "  Downloading striprtf-0.0.27-py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading striprtf-0.0.27-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: striprtf\n",
            "Successfully installed striprtf-0.0.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the file given is in rtf format\n",
        "# we need json data\n",
        "# hence need to convert the rtf to json and extract the data\n",
        "# into python dictonary format\n",
        "\n",
        "import json\n",
        "from striprtf.striprtf import rtf_to_text\n",
        "import logging\n",
        "\n",
        "# Initializing the logger\n",
        "logging.basicConfig()\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Here I define a fucntion to parse json from rtf\n",
        "\n",
        "def parse_json_from_rtf(rtf_file_path):\n",
        "  try:\n",
        "    # read the rtf file\n",
        "    with open(file_path, 'r', encoding = 'utf-8') as rtf_file:\n",
        "      rtf_content = rtf_file.read()\n",
        "\n",
        "    # converting the rtf to plain text\n",
        "    plain_text = rtf_to_text(rtf_content)\n",
        "\n",
        "    # load json from plain text\n",
        "    json_data = json.loads(plain_text)\n",
        "\n",
        "    # returning the json data\n",
        "    return json_data\n",
        "  except json.JSONDecodeError as e :\n",
        "    print(f\"Error parsing JSON from RTF: {e}\")\n",
        "    return None\n",
        "  except Exception as e:\n",
        "    print(f\"An error occured: {e}\")\n",
        "    return None\n",
        "\n",
        "# giving file path to the function and calling the function\n",
        "file_path = '/content/Screening_Test_DS/algoparams_from_ui.json.rtf'\n",
        "json_data = parse_json_from_rtf(file_path)\n",
        "\n",
        "if json_data:\n",
        "  print(\"Parsed JSON data:\")\n",
        "  print(json.dumps(json_data, indent = 4))\n",
        "else:\n",
        "  print(\"Failed to parse JSON\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8q2e0YWYnk_",
        "outputId": "f4dc635d-5bdb-4567-e764-4f841181b4dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed JSON data:\n",
            "{\n",
            "    \"session_name\": \"test\",\n",
            "    \"session_description\": \"test\",\n",
            "    \"design_state_data\": {\n",
            "        \"session_info\": {\n",
            "            \"project_id\": \"1\",\n",
            "            \"experiment_id\": \"kkkk-11\",\n",
            "            \"dataset\": \"iris_modified.csv\",\n",
            "            \"session_name\": \"test\",\n",
            "            \"session_description\": \"test\"\n",
            "        },\n",
            "        \"target\": {\n",
            "            \"prediction_type\": \"Regression\",\n",
            "            \"target\": \"petal_width\",\n",
            "            \"type\": \"regression\",\n",
            "            \"partitioning\": true\n",
            "        },\n",
            "        \"train\": {\n",
            "            \"policy\": \"Split the dataset\",\n",
            "            \"time_variable\": \"sepal_length\",\n",
            "            \"sampling_method\": \"No sampling(whole data)\",\n",
            "            \"split\": \"Randomly\",\n",
            "            \"k_fold\": false,\n",
            "            \"train_ratio\": 0,\n",
            "            \"random_seed\": 0\n",
            "        },\n",
            "        \"metrics\": {\n",
            "            \"optomize_model_hyperparameters_for\": \"AUC\",\n",
            "            \"optimize_threshold_for\": \"F1 Score\",\n",
            "            \"compute_lift_at\": 0,\n",
            "            \"cost_matrix_gain_for_true_prediction_true_result\": 1,\n",
            "            \"cost_matrix_gain_for_true_prediction_false_result\": 0,\n",
            "            \"cost_matrix_gain_for_false_prediction_true_result\": 0,\n",
            "            \"cost_matrix_gain_for_false_prediction_false_result\": 0\n",
            "        },\n",
            "        \"feature_handling\": {\n",
            "            \"sepal_length\": {\n",
            "                \"feature_name\": \"sepal_length\",\n",
            "                \"is_selected\": true,\n",
            "                \"feature_variable_type\": \"numerical\",\n",
            "                \"feature_details\": {\n",
            "                    \"numerical_handling\": \"Keep as regular numerical feature\",\n",
            "                    \"rescaling\": \"No rescaling\",\n",
            "                    \"make_derived_feats\": false,\n",
            "                    \"missing_values\": \"Impute\",\n",
            "                    \"impute_with\": \"Average of values\",\n",
            "                    \"impute_value\": 0\n",
            "                }\n",
            "            },\n",
            "            \"sepal_width\": {\n",
            "                \"feature_name\": \"sepal_width\",\n",
            "                \"is_selected\": true,\n",
            "                \"feature_variable_type\": \"numerical\",\n",
            "                \"feature_details\": {\n",
            "                    \"numerical_handling\": \"Keep as regular numerical feature\",\n",
            "                    \"rescaling\": \"No rescaling\",\n",
            "                    \"make_derived_feats\": false,\n",
            "                    \"missing_values\": \"Impute\",\n",
            "                    \"impute_with\": \"custom\",\n",
            "                    \"impute_value\": -1\n",
            "                }\n",
            "            },\n",
            "            \"petal_length\": {\n",
            "                \"feature_name\": \"petal_length\",\n",
            "                \"is_selected\": true,\n",
            "                \"feature_variable_type\": \"numerical\",\n",
            "                \"feature_details\": {\n",
            "                    \"numerical_handling\": \"Keep as regular numerical feature\",\n",
            "                    \"rescaling\": \"No rescaling\",\n",
            "                    \"make_derived_feats\": false,\n",
            "                    \"missing_values\": \"Impute\",\n",
            "                    \"impute_with\": \"Average of values\",\n",
            "                    \"impute_value\": 0\n",
            "                }\n",
            "            },\n",
            "            \"petal_width\": {\n",
            "                \"feature_name\": \"petal_width\",\n",
            "                \"is_selected\": true,\n",
            "                \"feature_variable_type\": \"numerical\",\n",
            "                \"feature_details\": {\n",
            "                    \"numerical_handling\": \"Keep as regular numerical feature\",\n",
            "                    \"rescaling\": \"No rescaling\",\n",
            "                    \"make_derived_feats\": false,\n",
            "                    \"missing_values\": \"Impute\",\n",
            "                    \"impute_with\": \"custom\",\n",
            "                    \"impute_value\": -2\n",
            "                }\n",
            "            },\n",
            "            \"species\": {\n",
            "                \"feature_name\": \"species\",\n",
            "                \"is_selected\": true,\n",
            "                \"feature_variable_type\": \"text\",\n",
            "                \"feature_details\": {\n",
            "                    \"text_handling\": \"Tokenize and hash\",\n",
            "                    \"hash_columns\": 0\n",
            "                }\n",
            "            }\n",
            "        },\n",
            "        \"feature_generation\": {\n",
            "            \"linear_interactions\": [\n",
            "                [\n",
            "                    \"petal_length\",\n",
            "                    \"sepal_width\"\n",
            "                ]\n",
            "            ],\n",
            "            \"linear_scalar_type\": \"robust\",\n",
            "            \"polynomial_interactions\": [\n",
            "                \"petal_length/sepal_width\",\n",
            "                \"petal_width/species\"\n",
            "            ],\n",
            "            \"explicit_pairwise_interactions\": [\n",
            "                \"sepal_width/sepal_length\",\n",
            "                \"petal_width/sepal_length\"\n",
            "            ]\n",
            "        },\n",
            "        \"feature_reduction\": {\n",
            "            \"feature_reduction_method\": \"Tree-based\",\n",
            "            \"num_of_features_to_keep\": \"4\",\n",
            "            \"num_of_trees\": \"5\",\n",
            "            \"depth_of_trees\": \"6\"\n",
            "        },\n",
            "        \"hyperparameters\": {\n",
            "            \"stratergy\": \"Grid Search\",\n",
            "            \"shuffle_grid\": true,\n",
            "            \"random_state\": 1,\n",
            "            \"max_iterations\": 2,\n",
            "            \"max_search_time\": 3,\n",
            "            \"parallelism\": 5,\n",
            "            \"cross_validation_stratergy\": \"Time-based K-fold(with overlap)\",\n",
            "            \"num_of_folds\": 6,\n",
            "            \"split_ratio\": 0,\n",
            "            \"stratified\": true\n",
            "        },\n",
            "        \"weighting_stratergy\": {\n",
            "            \"weighting_stratergy_method\": \"Sample weights\",\n",
            "            \"weighting_stratergy_weight_variable\": \"petal_length\"\n",
            "        },\n",
            "        \"probability_calibration\": {\n",
            "            \"probability_calibration_method\": \"Sigmoid - Platt Scaling\"\n",
            "        },\n",
            "        \"algorithms\": {\n",
            "            \"RandomForestClassifier\": {\n",
            "                \"model_name\": \"Random Forest Classifier\",\n",
            "                \"is_selected\": false,\n",
            "                \"min_trees\": 10,\n",
            "                \"max_trees\": 30,\n",
            "                \"feature_sampling_statergy\": \"Default\",\n",
            "                \"min_depth\": 20,\n",
            "                \"max_depth\": 30,\n",
            "                \"min_samples_per_leaf_min_value\": 5,\n",
            "                \"min_samples_per_leaf_max_value\": 50,\n",
            "                \"parallelism\": 0\n",
            "            },\n",
            "            \"RandomForestRegressor\": {\n",
            "                \"model_name\": \"Random Forest Regressor\",\n",
            "                \"is_selected\": true,\n",
            "                \"min_trees\": 10,\n",
            "                \"max_trees\": 20,\n",
            "                \"feature_sampling_statergy\": \"Default\",\n",
            "                \"min_depth\": 20,\n",
            "                \"max_depth\": 25,\n",
            "                \"min_samples_per_leaf_min_value\": 5,\n",
            "                \"min_samples_per_leaf_max_value\": 10,\n",
            "                \"parallelism\": 0\n",
            "            },\n",
            "            \"GBTClassifier\": {\n",
            "                \"model_name\": \"Gradient Boosted Trees\",\n",
            "                \"is_selected\": false,\n",
            "                \"num_of_BoostingStages\": [\n",
            "                    67,\n",
            "                    89\n",
            "                ],\n",
            "                \"feature_sampling_statergy\": \"Fixed number\",\n",
            "                \"learningRate\": [],\n",
            "                \"use_deviance\": true,\n",
            "                \"use_exponential\": false,\n",
            "                \"fixed_number\": 22,\n",
            "                \"min_subsample\": 1,\n",
            "                \"max_subsample\": 2,\n",
            "                \"min_stepsize\": 0.1,\n",
            "                \"max_stepsize\": 0.5,\n",
            "                \"min_iter\": 20,\n",
            "                \"max_iter\": 40,\n",
            "                \"min_depth\": 5,\n",
            "                \"max_depth\": 7\n",
            "            },\n",
            "            \"GBTRegressor\": {\n",
            "                \"model_name\": \"Gradient Boosted Trees\",\n",
            "                \"is_selected\": false,\n",
            "                \"num_of_BoostingStages\": [\n",
            "                    67,\n",
            "                    89\n",
            "                ],\n",
            "                \"feature_sampling_statergy\": \"Fixed number\",\n",
            "                \"use_deviance\": true,\n",
            "                \"use_exponential\": false,\n",
            "                \"fixed_number\": 22,\n",
            "                \"min_subsample\": 1,\n",
            "                \"max_subsample\": 2,\n",
            "                \"min_stepsize\": 0.1,\n",
            "                \"max_stepsize\": 0.5,\n",
            "                \"min_iter\": 20,\n",
            "                \"max_iter\": 40,\n",
            "                \"min_depth\": 5,\n",
            "                \"max_depth\": 7\n",
            "            },\n",
            "            \"LinearRegression\": {\n",
            "                \"model_name\": \"LinearRegression\",\n",
            "                \"is_selected\": false,\n",
            "                \"parallelism\": 2,\n",
            "                \"min_iter\": 30,\n",
            "                \"max_iter\": 50,\n",
            "                \"min_regparam\": 0.5,\n",
            "                \"max_regparam\": 0.8,\n",
            "                \"min_elasticnet\": 0.5,\n",
            "                \"max_elasticnet\": 0.8\n",
            "            },\n",
            "            \"LogisticRegression\": {\n",
            "                \"model_name\": \"LogisticRegression\",\n",
            "                \"is_selected\": false,\n",
            "                \"parallelism\": 2,\n",
            "                \"min_iter\": 30,\n",
            "                \"max_iter\": 50,\n",
            "                \"min_regparam\": 0.5,\n",
            "                \"max_regparam\": 0.8,\n",
            "                \"min_elasticnet\": 0.5,\n",
            "                \"max_elasticnet\": 0.8\n",
            "            },\n",
            "            \"RidgeRegression\": {\n",
            "                \"model_name\": \"RidgeRegression\",\n",
            "                \"is_selected\": false,\n",
            "                \"regularization_term\": \"Specify values to test\",\n",
            "                \"min_iter\": 30,\n",
            "                \"max_iter\": 50,\n",
            "                \"min_regparam\": 0.5,\n",
            "                \"max_regparam\": 0.8\n",
            "            },\n",
            "            \"LassoRegression\": {\n",
            "                \"model_name\": \"Lasso Regression\",\n",
            "                \"is_selected\": false,\n",
            "                \"regularization_term\": \"Specify values to test\",\n",
            "                \"min_iter\": 30,\n",
            "                \"max_iter\": 50,\n",
            "                \"min_regparam\": 0.5,\n",
            "                \"max_regparam\": 0.8\n",
            "            },\n",
            "            \"ElasticNetRegression\": {\n",
            "                \"model_name\": \"Lasso Regression\",\n",
            "                \"is_selected\": false,\n",
            "                \"regularization_term\": \"Specify values to test\",\n",
            "                \"min_iter\": 30,\n",
            "                \"max_iter\": 50,\n",
            "                \"min_regparam\": 0.5,\n",
            "                \"max_regparam\": 0.8,\n",
            "                \"min_elasticnet\": 0.5,\n",
            "                \"max_elasticnet\": 0.8\n",
            "            },\n",
            "            \"xg_boost\": {\n",
            "                \"model_name\": \"XG Boost\",\n",
            "                \"is_selected\": false,\n",
            "                \"use_gradient_boosted_tree\": true,\n",
            "                \"dart\": true,\n",
            "                \"tree_method\": \"\",\n",
            "                \"random_state\": 0,\n",
            "                \"max_num_of_trees\": 0,\n",
            "                \"early_stopping\": true,\n",
            "                \"early_stopping_rounds\": 2,\n",
            "                \"max_depth_of_tree\": [\n",
            "                    56,\n",
            "                    89\n",
            "                ],\n",
            "                \"learningRate\": [\n",
            "                    89,\n",
            "                    76\n",
            "                ],\n",
            "                \"l1_regularization\": [\n",
            "                    77\n",
            "                ],\n",
            "                \"l2_regularization\": [\n",
            "                    78\n",
            "                ],\n",
            "                \"gamma\": [\n",
            "                    68\n",
            "                ],\n",
            "                \"min_child_weight\": [\n",
            "                    67\n",
            "                ],\n",
            "                \"sub_sample\": [\n",
            "                    67\n",
            "                ],\n",
            "                \"col_sample_by_tree\": [\n",
            "                    67\n",
            "                ],\n",
            "                \"replace_missing_values\": false,\n",
            "                \"parallelism\": 0\n",
            "            },\n",
            "            \"DecisionTreeRegressor\": {\n",
            "                \"model_name\": \"Decision Tree\",\n",
            "                \"is_selected\": false,\n",
            "                \"min_depth\": 4,\n",
            "                \"max_depth\": 7,\n",
            "                \"use_gini\": false,\n",
            "                \"use_entropy\": true,\n",
            "                \"min_samples_per_leaf\": [\n",
            "                    12,\n",
            "                    6\n",
            "                ],\n",
            "                \"use_best\": true,\n",
            "                \"use_random\": true\n",
            "            },\n",
            "            \"DecisionTreeClassifier\": {\n",
            "                \"model_name\": \"Decision Tree\",\n",
            "                \"is_selected\": false,\n",
            "                \"min_depth\": 4,\n",
            "                \"max_depth\": 7,\n",
            "                \"use_gini\": false,\n",
            "                \"use_entropy\": true,\n",
            "                \"min_samples_per_leaf\": [\n",
            "                    12,\n",
            "                    6\n",
            "                ],\n",
            "                \"use_best\": true,\n",
            "                \"use_random\": true\n",
            "            },\n",
            "            \"SVM\": {\n",
            "                \"model_name\": \"Support Vector Machine\",\n",
            "                \"is_selected\": false,\n",
            "                \"linear_kernel\": true,\n",
            "                \"rep_kernel\": true,\n",
            "                \"polynomial_kernel\": true,\n",
            "                \"sigmoid_kernel\": true,\n",
            "                \"c_value\": [\n",
            "                    566,\n",
            "                    79\n",
            "                ],\n",
            "                \"auto\": true,\n",
            "                \"scale\": true,\n",
            "                \"custom_gamma_values\": true,\n",
            "                \"tolerance\": 7,\n",
            "                \"max_iterations\": 7\n",
            "            },\n",
            "            \"SGD\": {\n",
            "                \"model_name\": \"Stochastic Gradient Descent\",\n",
            "                \"is_selected\": false,\n",
            "                \"use_logistics\": true,\n",
            "                \"use_modified_hubber_loss\": false,\n",
            "                \"max_iterations\": false,\n",
            "                \"tolerance\": 56,\n",
            "                \"use_l1_regularization\": \"on\",\n",
            "                \"use_l2_regularization\": \"on\",\n",
            "                \"use_elastic_net_regularization\": true,\n",
            "                \"alpha_value\": [\n",
            "                    79,\n",
            "                    56\n",
            "                ],\n",
            "                \"parallelism\": 1\n",
            "            },\n",
            "            \"KNN\": {\n",
            "                \"model_name\": \"KNN\",\n",
            "                \"is_selected\": false,\n",
            "                \"k_value\": [\n",
            "                    78\n",
            "                ],\n",
            "                \"distance_weighting\": true,\n",
            "                \"neighbour_finding_algorithm\": \"Automatic\",\n",
            "                \"random_state\": 0,\n",
            "                \"p_value\": 0\n",
            "            },\n",
            "            \"extra_random_trees\": {\n",
            "                \"model_name\": \"Extra Random Trees\",\n",
            "                \"is_selected\": false,\n",
            "                \"num_of_trees\": [\n",
            "                    45,\n",
            "                    489\n",
            "                ],\n",
            "                \"feature_sampling_statergy\": \"Square root and Logarithm\",\n",
            "                \"max_depth\": [\n",
            "                    12,\n",
            "                    45\n",
            "                ],\n",
            "                \"min_samples_per_leaf\": [\n",
            "                    78,\n",
            "                    56\n",
            "                ],\n",
            "                \"parallelism\": 3\n",
            "            },\n",
            "            \"neural_network\": {\n",
            "                \"model_name\": \"Neural Network\",\n",
            "                \"is_selected\": false,\n",
            "                \"hidden_layer_sizes\": [\n",
            "                    67,\n",
            "                    89\n",
            "                ],\n",
            "                \"activation\": \"\",\n",
            "                \"alpha_value\": 0,\n",
            "                \"max_iterations\": 0,\n",
            "                \"convergence_tolerance\": 0,\n",
            "                \"early_stopping\": true,\n",
            "                \"solver\": \"ADAM\",\n",
            "                \"shuffle_data\": true,\n",
            "                \"initial_learning_rate\": 0,\n",
            "                \"automatic_batching\": true,\n",
            "                \"beta_1\": 0,\n",
            "                \"beta_2\": 0,\n",
            "                \"epsilon\": 0,\n",
            "                \"power_t\": 0,\n",
            "                \"momentum\": 0,\n",
            "                \"use_nesterov_momentum\": false\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: To read target and regression type\n",
        "# For this step I have already parsed the json data\n",
        "# Now I only need to read the data\n",
        "# I can do that by creating a fucntion for it\n",
        "\n",
        "# Here I have defined the function and given input as the json data\n",
        "# that I extracted before\n",
        "\n",
        "# Also the target and type are nested under the design state data\n",
        "# hence I will have to consider that\n",
        "\n",
        "def parse_target_and_regression(json_data):\n",
        "  try:\n",
        "    if \"design_state_data\" not in json_data and \"target\" not in json_data[\"design_state_data\"]:\n",
        "      raise ValueError(\"Json data is missing with keys 'design_state_data' and 'target'\")\n",
        "\n",
        "    # Now I need to navigate to the target in the dictionary\n",
        "    target_data = json_data[\"design_state_data\"][\"target\"]\n",
        "\n",
        "    target = target_data.get(\"target\")\n",
        "    regression_type = target_data.get(\"type\")\n",
        "\n",
        "    if target and regression_type:\n",
        "      logger.info(f\"Target: {target}\")\n",
        "      logger.info(f\"Regression Type: {regression_type}\")\n",
        "    else:\n",
        "      logging.error(\"Failed to parse target and type\")\n",
        "\n",
        "    # lets validate both target and type\n",
        "    if not target:\n",
        "      raise ValueError(\" 'target' filed missing in json data\")\n",
        "    if not regression_type:\n",
        "      raise ValueError(\" 'prediction_type' field missing in json data\")\n",
        "\n",
        "    return target, regression_type\n",
        "  except Exception as e:\n",
        "    logger.error(f\"Error parsing target and type : {e}\")\n",
        "    return None, None\n",
        "\n",
        "# Here I call the function\n",
        "target, regression_type = parse_target_and_regression(json_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98RE_A3JaneX",
        "outputId": "8960a97a-8994-448d-f223-9ef10d192ca3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Target: petal_width\n",
            "INFO:__main__:Regression Type: regression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2:\n",
        "# Here I have to read the csv as well hence I will be importing pandas\n",
        "# also from the parsed json data will look at the feature handling to\n",
        "# determine the imputation strategy for each feature and apply it to the dataframe\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# for this function I have given inputs as json data which is parsed data and dataframe\n",
        "\n",
        "def parse_and_impute_data(df, json_data):\n",
        "  try:\n",
        "    # I will have to extract the feature handling details\n",
        "    feature_handling = json_data[\"design_state_data\"][\"feature_handling\"]\n",
        "\n",
        "    # Here I will apply the imputation strategies\n",
        "    for feature_name, feature_info in feature_handling.items():\n",
        "      if feature_name in df.columns:\n",
        "        # getting the feature name\n",
        "        feature_details = feature_info.get(\"feature_details\", {})\n",
        "\n",
        "        # getting the impation method and imputation value\n",
        "        imputation_method = feature_details.get(\"impute_with\")\n",
        "        imputation_value = feature_details.get(\"impute_value\")\n",
        "\n",
        "        # validate the extracted values\n",
        "        print(f\"processing feature : {feature_name}\")\n",
        "        print(f\"Imputation method : {imputation_method}\")\n",
        "        print(f\"Imputation value : {imputation_value}\")\n",
        "\n",
        "        # The json data shows that there are two imputation that is average of values\n",
        "        # and custom. Hence lets apply those two methods of imputation\n",
        "        if imputation_method == \"Average of values\":\n",
        "            logger.info(f\"Imputing {feature_name} with mean value. \")\n",
        "            # applying mean imputation to dataframe\n",
        "            df[feature_name] = df[feature_name].fillna(df[feature_name].mean())\n",
        "        elif imputation_method == \"custom\":\n",
        "            logger.info(f\"Imputing {feature_name} with custom value: {imputation_value}\")\n",
        "            # applying custom imputation to dataframe\n",
        "            df[feature_name] = df[feature_name].fillna(imputation_value)\n",
        "        else:\n",
        "            logger.warning(f\"No valid imputation strategy for {feature_name}\")\n",
        "      else:\n",
        "        logger.warning(f\"Feature {feature_name} not found in dataset\")\n",
        "\n",
        "    return df\n",
        "  except Exception as e:\n",
        "    logger.error(f\"Error occured during the process: {e}\")\n",
        "    return None\n",
        "\n",
        "# Now lets specify the csv path\n",
        "# The path can be changed based on the current working directory\n",
        "csv_path = '/content/Screening_Test_DS/iris.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# lets call the function\n",
        "processed_df = parse_and_impute_data(df, json_data)\n",
        "\n",
        "if processed_df is not None:\n",
        "  logger.info(\"Data processing completed successfulyy\")\n",
        "  print(processed_df.head(20))\n",
        "else:\n",
        "  logger.error(\"Data processing failed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PbOc52kvYGp",
        "outputId": "cfe1136e-768d-4d75-d5c0-237078bae842"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Imputing sepal_length with mean value. \n",
            "INFO:__main__:Imputing sepal_width with custom value: -1\n",
            "INFO:__main__:Imputing petal_length with mean value. \n",
            "INFO:__main__:Imputing petal_width with custom value: -2\n",
            "WARNING:__main__:No valid imputation strategy for species\n",
            "INFO:__main__:Data processing completed successfulyy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing feature : sepal_length\n",
            "Imputation method : Average of values\n",
            "Imputation value : 0\n",
            "processing feature : sepal_width\n",
            "Imputation method : custom\n",
            "Imputation value : -1\n",
            "processing feature : petal_length\n",
            "Imputation method : Average of values\n",
            "Imputation value : 0\n",
            "processing feature : petal_width\n",
            "Imputation method : custom\n",
            "Imputation value : -2\n",
            "processing feature : species\n",
            "Imputation method : None\n",
            "Imputation value : None\n",
            "    sepal_length  sepal_width  petal_length  petal_width      species\n",
            "0            5.1          3.5           1.4          0.2  Iris-setosa\n",
            "1            4.9          3.0           1.4          0.2  Iris-setosa\n",
            "2            4.7          3.2           1.3          0.2  Iris-setosa\n",
            "3            4.6          3.1           1.5          0.2  Iris-setosa\n",
            "4            5.0          3.6           1.4          0.2  Iris-setosa\n",
            "5            5.4          3.9           1.7          0.4  Iris-setosa\n",
            "6            4.6          3.4           1.4          0.3  Iris-setosa\n",
            "7            5.0          3.4           1.5          0.2  Iris-setosa\n",
            "8            4.4          2.9           1.4          0.2  Iris-setosa\n",
            "9            4.9          3.1           1.5          0.1  Iris-setosa\n",
            "10           5.4          3.7           1.5          0.2  Iris-setosa\n",
            "11           4.8          3.4           1.6          0.2  Iris-setosa\n",
            "12           4.8          3.0           1.4          0.1  Iris-setosa\n",
            "13           4.3          3.0           1.1          0.1  Iris-setosa\n",
            "14           5.8          4.0           1.2          0.2  Iris-setosa\n",
            "15           5.7          4.4           1.5          0.4  Iris-setosa\n",
            "16           5.4          3.9           1.3          0.4  Iris-setosa\n",
            "17           5.1          3.5           1.4          0.3  Iris-setosa\n",
            "18           5.7          3.8           1.7          0.3  Iris-setosa\n",
            "19           5.1          3.8           1.5          0.3  Iris-setosa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3:\n",
        "# Here I have to compute feture reduction based on the input\n",
        "# Currently tree based feature reduction is used\n",
        "# I have to make sure to keep options for No reduction, corr with Target and PCA as well\n",
        "\n",
        "# lets import the necessary libraries for all the above options to work\n",
        "# pandas is already imported so no need to import again\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA  # library for PCA\n",
        "from sklearn.ensemble import RandomForestRegressor # library for Tree based\n",
        "from sklearn.feature_selection import mutual_info_regression # library for corr with target\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# if we check the csv the last column in the dataset is a non-numerical value\n",
        "# hence the reduction wont work\n",
        "# I will convert those non-numerical values using label encoding\n",
        "\n",
        "def label_encode_target(df):\n",
        "  # initializing the empty dict\n",
        "  label_encoders = {}\n",
        "  for col in df.select_dtypes(include =['object', 'category']).columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "    return df, label_encoders\n",
        "\n",
        "# considering the json data feature reduction falls under\n",
        "# design state data --> feature reduction -->  feature reduction method\n",
        "\n",
        "# here I have to apply feature reduction on the dataframe so\n",
        "# giving input as dataframe and json data\n",
        "def feature_reduction(df, json_data):\n",
        "  try:\n",
        "    reduction_method = json_data[\"design_state_data\"][\"feature_reduction\"][\"feature_reduction_method\"]\n",
        "    target_data = json_data[\"design_state_data\"][\"target\"][\"target\"]\n",
        "\n",
        "    # Encode the non-numerical columns\n",
        "    df, _ = label_encode_target(df)\n",
        "\n",
        "    # lets check each reduction method and apply it to the data\n",
        "    # first is no reduction method\n",
        "    if reduction_method == \"No Reduction\":\n",
        "      logger.info(\"No feature reduction applied\")\n",
        "      return df\n",
        "\n",
        "    # second is corr with target\n",
        "    elif reduction_method == \"Corr with Target\":\n",
        "      # check if target data is present\n",
        "      if target_data not in df.columns:\n",
        "        raise ValueError(f\"Target {target_data} not found in dataset\")\n",
        "\n",
        "      # if target data is found compute correlation\n",
        "      correlations = df.corrwith(df[target_data])\n",
        "\n",
        "      # Keep top 5 correlation features\n",
        "      top_features = correlations.abs().nlargest(5).index.tolist()\n",
        "      logger.info(f\"Selected top features : {top_features}\")\n",
        "\n",
        "      return df[top_features]\n",
        "\n",
        "    # third is tree-based\n",
        "    elif reduction_method == \"Tree-based\":\n",
        "      # convert the num of features, num of trees and depth of trees to integer\n",
        "      num_features = int(json_data[\"design_state_data\"][\"feature_reduction\"][\"num_of_features_to_keep\"])\n",
        "      num_trees = int(json_data[\"design_state_data\"][\"feature_reduction\"][\"num_of_trees\"])\n",
        "      max_depth = int(json_data[\"design_state_data\"][\"feature_reduction\"][\"depth_of_trees\"])\n",
        "\n",
        "      # check if target data is present\n",
        "      if target_data not in df.columns:\n",
        "        raise ValueError(f\"Target {target_data} not present in dataset\")\n",
        "\n",
        "      # now we initialize the X, y for random forest regressor\n",
        "      X = df.drop(columns = [target_data])\n",
        "      y = df[target_data]\n",
        "\n",
        "      # initialize the model\n",
        "      model = RandomForestRegressor(n_estimators= num_trees, max_depth = max_depth)\n",
        "\n",
        "      # fit the model\n",
        "      model.fit(X,y)\n",
        "\n",
        "      # get the important features from tree based\n",
        "      importances = model.feature_importances_\n",
        "\n",
        "      # get the feature importance dataframe\n",
        "      feature_importance_df = pd.DataFrame({\"feature\": X.columns, \"importance\": importances})\n",
        "\n",
        "      # get the top features\n",
        "      top_features = feature_importance_df.nlargest(num_features, \"importance\")[\"feature\"].tolist()\n",
        "      logger.info(f\"Selected top features: {top_features}\")\n",
        "\n",
        "      # return the dataframe\n",
        "      return df[top_features + [target_data]]\n",
        "\n",
        "    elif reduction_method == \"PCA\":\n",
        "      # convert the num features to integer\n",
        "      num_features = int(json_data[\"design_state_data\"][\"feature_reduction\"][\"num_of_features_to_keep\"])\n",
        "\n",
        "      # select the data for PCA\n",
        "      X = df.select_dtypes(include = [np.number]).drop(columns = [target_data], errors = \"ignore\")\n",
        "\n",
        "      # Initialize PCA\n",
        "      pca = PCA(n_components= num_features)\n",
        "\n",
        "      # fit PCA model\n",
        "      pca_result = pca.fit_transform(X)\n",
        "\n",
        "      # create dataframe for PCA components\n",
        "      pca_df = pd.DataFrame(data = pca_result, columns = [f\"PC{i + 1}\" for i in range(num_features)])\n",
        "\n",
        "      logger.info(f\"PCA applied, number of components : {num_features}\")\n",
        "\n",
        "      if target_data in df.columns:\n",
        "        pca_df[target_data] = df[target_data].values\n",
        "        return pca_df\n",
        "\n",
        "      else:\n",
        "        raise ValueError(f\"Invalid feature reduction method: {reduction_method}\")\n",
        "\n",
        "  except Exception as e:\n",
        "    logger.error(f\"Error occured during feature reduction: {e}\")\n",
        "    return None\n",
        "\n",
        "# csv file path\n",
        "csv_path = '/content/Screening_Test_DS/iris.csv'\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "reduced_df = feature_reduction(df, json_data)\n",
        "\n",
        "if reduced_df is not None:\n",
        "  logger.info(\"Feature reduction completed successfully\")\n",
        "  print(reduced_df.head(10))\n",
        "else:\n",
        "  logger.error(\"Feature reduction failed\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA9oGazn4HN_",
        "outputId": "f45a8c4a-fba1-48fb-ecd6-e3f87edc64ac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Selected top features: ['petal_length', 'species', 'sepal_width', 'sepal_length']\n",
            "INFO:__main__:Feature reduction completed successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   petal_length  species  sepal_width  sepal_length  petal_width\n",
            "0           1.4        0          3.5           5.1          0.2\n",
            "1           1.4        0          3.0           4.9          0.2\n",
            "2           1.3        0          3.2           4.7          0.2\n",
            "3           1.5        0          3.1           4.6          0.2\n",
            "4           1.4        0          3.6           5.0          0.2\n",
            "5           1.7        0          3.9           5.4          0.4\n",
            "6           1.4        0          3.4           4.6          0.3\n",
            "7           1.5        0          3.4           5.0          0.2\n",
            "8           1.4        0          2.9           4.4          0.2\n",
            "9           1.5        0          3.1           4.9          0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4:\n",
        "# Here the prediction type is given as regression\n",
        "# hence I will have to create the model objects that can only handle regression algorithms\n",
        "# The json data shows the regression algorithms used which are as follows:\n",
        "# RandomForestRegressor, GBTRegressor, LinearRegression, LogisticRegression,\n",
        "# RidgeRegression, LassoRegression, ElasticNetRegression, DecisionTreeRegressor.\n",
        "\n",
        "# Lets import the necessary libraries\n",
        "import inspect\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Now I will define a fucntion to parse the JSON and create models\n",
        "\n",
        "# Here we only require json data as input\n",
        "def create_models(json_data):\n",
        "  # We need to look at the prediction type and algorithms in the json data\n",
        "  # prediction type is located at design_state_data --> target --> prediction_type\n",
        "  # algorithms is located at design_state_data --> algorithms\n",
        "\n",
        "  prediction_type = json_data[\"design_state_data\"][\"target\"][\"prediction_type\"]\n",
        "  algorithms = json_data[\"design_state_data\"][\"algorithms\"]\n",
        "\n",
        "  # Now lets define the mappings for each model and create a dict\n",
        "  regression_model_mapping = {\n",
        "      \"RandomForestRegressor\" : RandomForestRegressor,\n",
        "      \"GBTRegressor\" : GradientBoostingRegressor,\n",
        "      \"LinearRegression\" : LinearRegression,\n",
        "      #\"LogisticRegression\" : LogisticRegression,\n",
        "      \"RidgeRegression\" : Ridge,\n",
        "      \"LassoRegression\" : Lasso,\n",
        "      \"ElasticNetRegression\" : ElasticNet,\n",
        "      \"DecisionTreeRegressor\" : DecisionTreeRegressor\n",
        "  }\n",
        "\n",
        "  # now lets initialize empty model dict\n",
        "  models = {}\n",
        "  if prediction_type == \"Regression\":\n",
        "\n",
        "    for algo_name, algo_details in algorithms.items():\n",
        "\n",
        "        # getting the model names\n",
        "        model_class = regression_model_mapping.get(algo_name)\n",
        "\n",
        "        # if model is not present in the model class then it is not a regression model\n",
        "        if not model_class:\n",
        "          logger.error(f\" Algorithm {algo_name} is not a regression model\")\n",
        "          continue\n",
        "\n",
        "        # creating valid params\n",
        "        valid_params = inspect.signature(model_class).parameters\n",
        "\n",
        "        # extracting the models\n",
        "        model_params = {\n",
        "                key : value for key, value in algorithms.items()\n",
        "                if key in valid_params\n",
        "            }\n",
        "        # Lets map JSON filed to scikit-learn models\n",
        "        if algo_name in regression_model_mapping:\n",
        "\n",
        "\n",
        "          # lets ininitialize the models\n",
        "          try :\n",
        "            models[algo_name] = model_class(**model_params)\n",
        "            logger.info(f\"Model {algo_name} created successfully : {models[algo_name]}\")\n",
        "          except TypeError as e:\n",
        "            print(f\"Error creating model {algo_name} : {e}\")\n",
        "            models[algo_name] = None\n",
        "\n",
        "        else:\n",
        "          logger.error(f\"Algorithm {algo_name} is not a regression model\")\n",
        "  else :\n",
        "    logger.error(f\"Invalid prediction type : {prediction_type}\")\n",
        "  return models\n",
        "\n",
        "# lets call the function\n",
        "models = create_models(json_data)\n",
        "\n",
        "print(\"\\nSummary of all models created : \")\n",
        "# displaying the created models\n",
        "for model_name, model_obj in models.items():\n",
        "  if model_obj is not None:\n",
        "    print(f\"Created model : {model_name}\")\n",
        "  else:\n",
        "    print(f\"Failed to create model : {model_name}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "425oz60KISwU",
        "outputId": "a3d30a3d-821e-49dd-a451-d915b2ab6b85"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__: Algorithm RandomForestClassifier is not a regression model\n",
            "INFO:__main__:Model RandomForestRegressor created successfully : RandomForestRegressor()\n",
            "ERROR:__main__: Algorithm GBTClassifier is not a regression model\n",
            "INFO:__main__:Model GBTRegressor created successfully : GradientBoostingRegressor()\n",
            "INFO:__main__:Model LinearRegression created successfully : LinearRegression()\n",
            "ERROR:__main__: Algorithm LogisticRegression is not a regression model\n",
            "INFO:__main__:Model RidgeRegression created successfully : Ridge()\n",
            "INFO:__main__:Model LassoRegression created successfully : Lasso()\n",
            "INFO:__main__:Model ElasticNetRegression created successfully : ElasticNet()\n",
            "ERROR:__main__: Algorithm xg_boost is not a regression model\n",
            "INFO:__main__:Model DecisionTreeRegressor created successfully : DecisionTreeRegressor()\n",
            "ERROR:__main__: Algorithm DecisionTreeClassifier is not a regression model\n",
            "ERROR:__main__: Algorithm SVM is not a regression model\n",
            "ERROR:__main__: Algorithm SGD is not a regression model\n",
            "ERROR:__main__: Algorithm KNN is not a regression model\n",
            "ERROR:__main__: Algorithm extra_random_trees is not a regression model\n",
            "ERROR:__main__: Algorithm neural_network is not a regression model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of all models created : \n",
            "Created model : RandomForestRegressor\n",
            "Created model : GBTRegressor\n",
            "Created model : LinearRegression\n",
            "Created model : RidgeRegression\n",
            "Created model : LassoRegression\n",
            "Created model : ElasticNetRegression\n",
            "Created model : DecisionTreeRegressor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5:\n",
        "# Now I have to run fit and predict on all the models in regression model mapping\n",
        "# first I need to create the param grid that is relevant to gridSearchCV\n",
        "\n",
        "# Let's create the param grid\n",
        "\n",
        "def create_param_grid(json_data):\n",
        "  # Here I will define json parameter names mapping to scikit learn parameter names\n",
        "  param_name_mapping = {\n",
        "      \"min_trees\" : \"n_estimators\",\n",
        "      \"max_trees\" : \"n_estimators\",\n",
        "      \"min_depth\" : \"max_depth\",\n",
        "      \"max_depth\" : \"max_depth\",\n",
        "      \"min_samples_per_leaf_min_value\" : \"min_samples_leaf\",\n",
        "      \"min_samples_per_leaf_max_value\" : \"min_samples_leaf\",\n",
        "      \"min_regparam\" : \"alpha\",\n",
        "      \"max_regparam\" : \"alpha\",\n",
        "      \"min_elasticnet\" : \"l1_ratio\",\n",
        "      \"max_elasticnet\" : \"l1_ratio\",\n",
        "  }\n",
        "\n",
        "  # Initilializing empty parameter grid\n",
        "  param_grid = {}\n",
        "\n",
        "  # access algorithms in JSON data\n",
        "  algorithms = json_data[\"design_state_data\"][\"algorithms\"]\n",
        "\n",
        "  for algo_name, algo_details in algorithms.items():\n",
        "\n",
        "    # initialize dict for this model params\n",
        "    model_params = {}\n",
        "\n",
        "    for json_param, sklearn_param in param_name_mapping.items():\n",
        "      if json_param in algo_details:\n",
        "        # create range of values if applicable\n",
        "        value = algo_details[json_param]\n",
        "        if isinstance(value,list) and len(value) == 2:\n",
        "          model_params[sklearn_param] = range(value[0] , value[1] + 1 )\n",
        "        else:\n",
        "          model_params[sklearn_param] = [value]\n",
        "\n",
        "    if model_params:\n",
        "      param_grid[algo_name] = model_params\n",
        "\n",
        "    # condition to remove 'alpha' from linear regression\n",
        "    if algo_name == \"LinearRegression\" and \"alpha\" in param_grid[algo_name]:\n",
        "      del param_grid[algo_name][\"alpha\"]\n",
        "\n",
        "    if algo_name == \"LinearRegression\" and \"l1_ratio\" in param_grid[algo_name]:\n",
        "      del param_grid[algo_name][\"l1_ratio\"]\n",
        "\n",
        "  return param_grid\n",
        "\n",
        "\n",
        "# now lets call the function and create param grid\n",
        "\n",
        "param_grid = create_param_grid(json_data)\n",
        "\n",
        "# display param grid\n",
        "print(\"\\n Param Grid: \")\n",
        "for model_name, grid in param_grid.items():\n",
        "  print(f\"{model_name} : {grid}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y08qPkBKKNgJ",
        "outputId": "8e30f6d0-67fd-4b70-df3d-a5e4afc75ef2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Param Grid: \n",
            "RandomForestClassifier : {'n_estimators': [30], 'max_depth': [30], 'min_samples_leaf': [50]}\n",
            "RandomForestRegressor : {'n_estimators': [20], 'max_depth': [25], 'min_samples_leaf': [10]}\n",
            "GBTClassifier : {'max_depth': [7]}\n",
            "GBTRegressor : {'max_depth': [7]}\n",
            "LinearRegression : {}\n",
            "LogisticRegression : {'alpha': [0.8], 'l1_ratio': [0.8]}\n",
            "RidgeRegression : {'alpha': [0.8]}\n",
            "LassoRegression : {'alpha': [0.8]}\n",
            "ElasticNetRegression : {'alpha': [0.8], 'l1_ratio': [0.8]}\n",
            "DecisionTreeRegressor : {'max_depth': [7]}\n",
            "DecisionTreeClassifier : {'max_depth': [7]}\n",
            "extra_random_trees : {'max_depth': range(12, 46)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6:\n",
        "# Running the grid search CV and getting all the metrics such mean squared error and R2 score\n",
        "# as we are using regression models only\n",
        "\n",
        "# importing libraries\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# defining the fit and predict function\n",
        "def fit_and_predict(models, param_grid, X_train, y_train, X_test, y_test):\n",
        "  best_model = None\n",
        "  best_score = float(\"inf\") # this as we want the lowest mse score for best model\n",
        "  # empty dict to store best metrics\n",
        "  best_metrics = {}\n",
        "  # empty dict to store results\n",
        "  results = {}\n",
        "\n",
        "  # iterating thorugh all modles in regression model mapping in step 4\n",
        "  for model_name, model in models.items():\n",
        "    if model is None:\n",
        "      continue\n",
        "\n",
        "    # get the parameter grid for the model\n",
        "    grid = param_grid.get(model_name, {})\n",
        "\n",
        "    # perform grid search Cv\n",
        "    grid_search = GridSearchCV(model, grid,cv=5, scoring = \"neg_mean_squared_error\", n_jobs = -1)\n",
        "\n",
        "    # fit the model to training data\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # best model\n",
        "    best_model_fit = grid_search.best_estimator_\n",
        "\n",
        "    # best params\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # predict with the best model\n",
        "    y_pred = best_model_fit.predict(X_test)\n",
        "\n",
        "    # calculate mean squared error\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # best r2 score\n",
        "    best_r2 = best_model_fit.score(X_test, y_test)\n",
        "\n",
        "\n",
        "    # save results for all the models\n",
        "    results[model_name] = {\n",
        "        \"Best model\" : best_model,\n",
        "        \"Best hyperparameters\" : best_params,\n",
        "        \"Mean square error\" : mse,\n",
        "        \"R2 score\" : best_r2,\n",
        "    }\n",
        "\n",
        "    # track best model based on mse\n",
        "    if mse < best_score:\n",
        "      best_score = mse\n",
        "      best_metrics = results[model_name]\n",
        "      best_model = best_model_fit\n",
        "\n",
        "  return results, best_metrics, best_model\n",
        "\n",
        "# now we need to give the data and convert it using train test split\n",
        "\n",
        "csv_path = '/content/Screening_Test_DS/iris.csv'\n",
        "\n",
        "# loading the data frame\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# converting the non-numerical columns\n",
        "# not imported library as it is already imported above\n",
        "le = LabelEncoder()\n",
        "df[\"species_encoded\"] = le.fit_transform(df[\"species\"])\n",
        "\n",
        "target_data = json_data[\"design_state_data\"][\"target\"][\"target\"]\n",
        "# splitting the data frame into X and y using target\n",
        "X = df[['sepal_length', 'sepal_width', 'petal_length', 'species_encoded']]\n",
        "y = df['petal_width']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# call the fit and predict function\n",
        "results, best_metrics, best_model = fit_and_predict(models, param_grid, X_train, y_train, X_test, y_test)\n",
        "\n",
        "# display results\n",
        "\n",
        "print(\"\\n Results for each model: \")\n",
        "\n",
        "for model_name, metrics in results.items():\n",
        "  print(f\"\\n{model_name}\")\n",
        "  for metric, value in metrics.items():\n",
        "    print(f\"{metric} : {value}\")\n",
        "\n",
        "print(f\"\\n Best model and performance metrics : \")\n",
        "print(f\"Best model : {best_model}\")\n",
        "print(f\"Best hyperparameters : {best_metrics['Best hyperparameters']}\")\n",
        "print(f\"Mean squared error : {best_metrics['Mean square error']}\")\n",
        "print(f\"Best R-squared : {best_metrics['R2 score']}\")\n"
      ],
      "metadata": {
        "id": "5ZpJJr4T6smm",
        "outputId": "741462d3-c308-40dc-865f-00b46886168a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Results for each model: \n",
            "\n",
            "RandomForestRegressor\n",
            "Best model : None\n",
            "Best hyperparameters : {'max_depth': 25, 'min_samples_leaf': 10, 'n_estimators': 20}\n",
            "Mean square error : 0.024283591141603122\n",
            "R2 score : 0.9617975632724872\n",
            "\n",
            "GBTRegressor\n",
            "Best model : RandomForestRegressor(max_depth=25, min_samples_leaf=10, n_estimators=20)\n",
            "Best hyperparameters : {'max_depth': 7}\n",
            "Mean square error : 0.042704815139070366\n",
            "R2 score : 0.932817679691721\n",
            "\n",
            "LinearRegression\n",
            "Best model : RandomForestRegressor(max_depth=25, min_samples_leaf=10, n_estimators=20)\n",
            "Best hyperparameters : {}\n",
            "Mean square error : 0.03011497295063442\n",
            "R2 score : 0.9526237556056373\n",
            "\n",
            "RidgeRegression\n",
            "Best model : RandomForestRegressor(max_depth=25, min_samples_leaf=10, n_estimators=20)\n",
            "Best hyperparameters : {'alpha': 0.8}\n",
            "Mean square error : 0.02962541534185668\n",
            "R2 score : 0.9533939173772116\n",
            "\n",
            "LassoRegression\n",
            "Best model : RandomForestRegressor(max_depth=25, min_samples_leaf=10, n_estimators=20)\n",
            "Best hyperparameters : {'alpha': 0.8}\n",
            "Mean square error : 0.2934232572283649\n",
            "R2 score : 0.5383926803378343\n",
            "\n",
            "ElasticNetRegression\n",
            "Best model : RandomForestRegressor(max_depth=25, min_samples_leaf=10, n_estimators=20)\n",
            "Best hyperparameters : {'alpha': 0.8, 'l1_ratio': 0.8}\n",
            "Mean square error : 0.22253727096228051\n",
            "R2 score : 0.6499090285338802\n",
            "\n",
            "DecisionTreeRegressor\n",
            "Best model : RandomForestRegressor(max_depth=25, min_samples_leaf=10, n_estimators=20)\n",
            "Best hyperparameters : {'max_depth': 7}\n",
            "Mean square error : 0.04146440329218107\n",
            "R2 score : 0.9347690696167335\n",
            "\n",
            " Best model and performance metrics : \n",
            "Best model : RandomForestRegressor(max_depth=25, min_samples_leaf=10, n_estimators=20)\n",
            "Best hyperparameters : {'max_depth': 25, 'min_samples_leaf': 10, 'n_estimators': 20}\n",
            "Mean squared error : 0.024283591141603122\n",
            "Best R-squared : 0.9617975632724872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GuwZGcA-IoxH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}